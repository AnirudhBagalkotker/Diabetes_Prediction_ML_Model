{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj_r89FzT41w"
      },
      "source": [
        "# **BITS F464 - Semester 1 - MACHINE LEARNING**\n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "**ASSIGNMENT 1 - LINEAR MODELS FOR REGRESSION AND CLASSIFICATION**\n",
        "--------------------------------------------------------------------------------\n",
        "***Team number: 13***\n",
        "\n",
        "---\n",
        "***Team Members: ANIRUDH BAGALKOTKER, KARTIK PANDEY, ADWAIT KULKARNI, JOY SINHA, PIYUSH JAJRA***\n",
        "\n",
        "---\n",
        "***IDs: 2021A7PS2682H, 2021A7PS2574H, 2021A7PS2995H, 2021A8PS1606H, 2021B4A72969H***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_duS0Zn17c3"
      },
      "source": [
        "This assignment aims to identify the differences between three sets of Machine Learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT-dTtra2h2n"
      },
      "source": [
        "# **_1. Dataset Generation_**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UsOVUj22wrz"
      },
      "source": [
        "You are given a sample Diabetes dataset. Using this, please develop your own dataset consisting of 500 records. You can use the given code to generate your own dataset. Submit the generated dataset as a .csv file along with your python notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8uONwSjNSc-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from sdv.datasets.local import load_csvs\n",
        "from sdv.metadata import SingleTableMetadata\n",
        "from sdv.lite import SingleTablePreset\n",
        "import warnings\n",
        "warnings.filterwarnings( \"ignore\" )\n",
        "\n",
        "# Getting the current directory using os.path and loading the csv file with the sample diabetes dataset\n",
        "folderName = os.getcwd()\n",
        "datasets = load_csvs(folder_name=folderName)\n",
        "real_data = datasets[\"diabetes\"]\n",
        "\n",
        "# Generating metadata for the sample dataset\n",
        "metadata = SingleTableMetadata()\n",
        "metadata.detect_from_csv(filepath=folderName + \"/diabetes.csv\")\n",
        "\n",
        "# Visualizing the metadata and print it\n",
        "real_data.head()\n",
        "metadata.visualize()\n",
        "print(\"\\n\")\n",
        "print(metadata.to_dict())\n",
        "\n",
        "# Initializing a SingleTablePreset object with the metadata and fitting the synthesizer and sampling with the real_data input.\n",
        "synthesizer = SingleTablePreset(metadata, name=\"FAST_ML\")\n",
        "synthesizer.fit(data=real_data)\n",
        "\n",
        "# Generating 500 rows of synthetic data using the synthesizer and saving it as a csv and the synthesizer as a pkl\n",
        "rows = 500\n",
        "synthetic_data = synthesizer.sample(num_rows=rows)\n",
        "synthetic_data.to_csv(\"synthetic_diabetes.csv\", index=False)\n",
        "# synthesizer.save(\"diabetes.pkl\")\n",
        "print(\"\\nSynthetic data generated.\\n\")\n",
        "print(synthetic_data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDu7bwbNRhaK"
      },
      "source": [
        "# ***2. Preprocess and perform exploratory data analysis of the dataset obtained***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAd4cNwERr90"
      },
      "outputs": [],
      "source": [
        "from sdv.evaluation.single_table import evaluate_quality\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "import random\n",
        "\n",
        "# Preprocessing of the Synthetic Data\n",
        "\n",
        "# Handle missing values (if any) by replacing them with the mean\n",
        "synthetic_data.fillna(synthetic_data.mean(), inplace=True)\n",
        "\n",
        "print(\"\\nSynthetic data Preprocessed.\\n\")\n",
        "\n",
        "# Exploratory Data Analysis of the Synthetic Data\n",
        "print(\"\\nEDA for Synthetic data.\\n\")\n",
        "\n",
        "# Display the Outcomes and its mean\n",
        "print(synthetic_data['Outcome'].value_counts())\n",
        "print(\"\\n\")\n",
        "print(synthetic_data.groupby('Outcome').mean())\n",
        "print(\"\\n\")\n",
        "\n",
        "# Display basic statistics\n",
        "print(synthetic_data.describe())\n",
        "print(\"\\n\")\n",
        "\n",
        "# Check data types and missing values\n",
        "print(synthetic_data.info())\n",
        "print(\"\\n\")\n",
        "\n",
        "# Calculate and visualize correlations between numeric columns using cluster maps and box plots using seaborn\n",
        "\n",
        "# Calculate correlations\n",
        "correlation_matrix = synthetic_data.corr()\n",
        "\n",
        "# # Plot clustermap\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# sns.clustermap(correlation_matrix, cmap=\"RdBu\", center=0, cbar=True, annot=True)\n",
        "# plt.title(\"Correlation Clustermap\")\n",
        "# plt.show()\n",
        "\n",
        "# # Box plot\n",
        "# plt.figure(figsize=(10, 10))\n",
        "# sns.boxplot(x=\"Outcome\", y=\"Glucose\", data=synthetic_data)\n",
        "# plt.xlabel(\"Outcome\")\n",
        "# plt.ylabel(\"Glucose\")\n",
        "# plt.title(\"Box Plot of Glucose by Outcome\")\n",
        "# plt.show()\n",
        "\n",
        "# Evaluating the quality of the synthetic data using sdv\n",
        "quality_report = evaluate_quality(real_data, synthetic_data, metadata)\n",
        "quality_report.get_visualization(\"Column Shapes\")\n",
        "\n",
        "# Save the Synthetic Data and the Synthesizing Model after preprocessing and evaluation\n",
        "synthetic_data.to_csv(\"synthetic_diabetes.csv\", index=False)\n",
        "synthesizer.save(\"diabetes.pkl\")\n",
        "\n",
        "# Separating the features and target\n",
        "target = synthetic_data[\"Outcome\"]\n",
        "features = synthetic_data.drop(columns=\"Outcome\", axis = 1)\n",
        "\n",
        "# Normalization and Standardization\n",
        "\n",
        "# Normalizing the data\n",
        "# features = (features - features.min()) / (features.max() - features.min())\n",
        "\n",
        "# Standardizing the data\n",
        "features = (features - features.mean()) / features.std()\n",
        "\n",
        "# Splitting the data into training (80%) and test (20%)\n",
        "total_samples = len(features)\n",
        "train_samples = int(0.8 * total_samples)\n",
        "\n",
        "# Shuffle the indices to randomize the data\n",
        "indices = np.arange(total_samples)\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "# Split the indices into training and test sets\n",
        "train_indices = indices[:train_samples]\n",
        "test_indices = indices[train_samples:]\n",
        "\n",
        "# Create training and test data\n",
        "x_tr = features.iloc[train_indices]\n",
        "y_tr = target.iloc[train_indices]\n",
        "x_te = features.iloc[test_indices]\n",
        "y_te = target.iloc[test_indices]\n",
        "\n",
        "# Accuracy Score functions\n",
        "\n",
        "def accuracy_score(y_pred, y_true):\n",
        "\t\"\"\"\n",
        "\tThe accuracy_score function takes in two arrays of labels and returns the fraction\n",
        "\tof time that they are equal. This is known as the accuracy score, or more commonly,\n",
        "\tthe classification rate. The function can also take an optional third parameter to specify \n",
        "\ta normalization method for when there are unequal numbers of predictions between classes.\n",
        "\n",
        "\t:param y_pred: Input the predicted values of y\n",
        "\t:param y_true: Pass in the actual labels of the data and y_pred is used to pass in our predicted labels\n",
        "\t:return: The fraction of correct predictions\n",
        "\t\"\"\"\n",
        "\n",
        "\tnum_correct = np.sum(y_true == y_pred)\n",
        "\tnum_total = len(y_true)\n",
        "\n",
        "\treturn (num_correct / num_total) * 100\n",
        "\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    # Calculate true positives, false positives, false negatives\n",
        "    tp = sum(1 for i in range(len(y_true)) if y_true[i] == 1 and y_pred[i] == 1)\n",
        "    fp = sum(1 for i in range(len(y_true)) if y_true[i] == 0 and y_pred[i] == 1)\n",
        "    fn = sum(1 for i in range(len(y_true)) if y_true[i] == 1 and y_pred[i] == 0)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score\n",
        "    precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
        "\n",
        "    return f1 * 100\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y81SQxOrRso5"
      },
      "source": [
        "# ***3. Comparison of Stochastic Gradient Descent and Batch Gradient Descent using Linear Regression***\n",
        "\n",
        "## **_Stochastic Gradient Descent_**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQ8CVHsER-Mh"
      },
      "outputs": [],
      "source": [
        "X=synthetic_data.iloc[:,:-1].values\n",
        "y=synthetic_data.iloc[:,-1].values\n",
        "\n",
        "def scale_features(X):\n",
        "    X_scaled = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
        "    return X_scaled\n",
        "\n",
        "# Scale features\n",
        "X_scaled = scale_features(X)\n",
        "X = np.column_stack((np.ones(len(X_scaled)), X_scaled))\n",
        "\n",
        "def train_test_split(X, y, test_ratio=0.2):\n",
        "    num_samples = len(X)\n",
        "    num_test_samples = int(test_ratio * num_samples)\n",
        "    test_indices = random.sample(range(num_samples), num_test_samples)\n",
        "    train_indices = [i for i in range(num_samples) if i not in test_indices]\n",
        "    return X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n",
        "\n",
        "X_train, X_test, y_train, y_test= train_test_split(X,y, test_ratio=0.2)\n",
        "\n",
        "def predict(X, weights):\n",
        "    y_pred=X.dot(weights)\n",
        "    return y_pred\n",
        "\n",
        "def gradient(X,y,weights):\n",
        "    y_pred=predict(X,weights)\n",
        "    grad=2*X.T.dot(y_pred-y)\n",
        "    return grad\n",
        "\n",
        "def stochastic_gradient_descent(X,y,learning_rate=0.01, num_epochs=1000):\n",
        "    num_samples, num_features=X.shape\n",
        "    weights=np.random.rand(num_features)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        np.random.seed(42)  # for reproducibility\n",
        "        shuffled_indices = np.random.permutation(len(X))\n",
        "        X_shuffled = X[shuffled_indices]\n",
        "        y_shuffled = y[shuffled_indices]\n",
        "        for i in range(0,num_samples):\n",
        "            grad=gradient(X_shuffled,y_shuffled, weights)\n",
        "            weights-= learning_rate*grad/num_samples\n",
        "    return weights\n",
        "\n",
        "learning_rate=0.001\n",
        "num_epochs=1000\n",
        "\n",
        "weights=stochastic_gradient_descent(X_train,y_train,learning_rate, num_epochs)\n",
        "print(\"Weights:\", weights)\n",
        "\n",
        "y_pred = predict(X_train, weights)\n",
        "y_pred = np.round(y_pred).astype(int)\n",
        "\n",
        "accuracy = calculate_metrics(y_train,y_pred)\n",
        "print(\"\\nThe Accuracy Score of Training Data: \", accuracy)\n",
        "\n",
        "y_pred = predict(X_test, weights)\n",
        "y_pred = np.round(y_pred).astype(int)\n",
        "\n",
        "accuracy = calculate_metrics(y_test,y_pred)\n",
        "print(\"\\nThe Accuracy Score of Test Data: \", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqIC34nWR-n7"
      },
      "source": [
        "## **_Batch Gradient Descent_**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hA5FysjSBzj"
      },
      "outputs": [],
      "source": [
        "X=synthetic_data.iloc[:,:-1].values\n",
        "y=synthetic_data.iloc[:,-1].values\n",
        "\n",
        "def scale_features(X):\n",
        "    X_scaled = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
        "    return X_scaled\n",
        "\n",
        "# Scale features\n",
        "X_scaled = scale_features(X)\n",
        "X = np.column_stack((np.ones(len(X_scaled)), X_scaled))\n",
        "\n",
        "def train_test_split(X, y, test_ratio=0.2):\n",
        "    num_samples = len(X)\n",
        "    num_test_samples = int(test_ratio * num_samples)\n",
        "    test_indices = random.sample(range(num_samples), num_test_samples)\n",
        "    train_indices = [i for i in range(num_samples) if i not in test_indices]\n",
        "    return X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_ratio=0.2)\n",
        "\n",
        "def predict(X, weights):\n",
        "    y_pred = X.dot(weights)\n",
        "    return y_pred\n",
        "\n",
        "def gradient(X, y, weights):\n",
        "    y_pred = predict(X, weights)\n",
        "    grad = 2 * X.T.dot(y_pred - y)\n",
        "    return grad\n",
        "\n",
        "def batch_gradient_descent(X, y, learning_rate=0.01, num_epochs=1000, batch_size=32):\n",
        "    num_samples, num_features = X.shape\n",
        "    weights = np.random.rand(num_features)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        np.random.seed(42)  # for reproducibility\n",
        "        shuffled_indices = np.random.permutation(len(X))\n",
        "        for i in range(0, num_samples, batch_size):\n",
        "            batch_indices = shuffled_indices[i:i + batch_size]\n",
        "            X_batch = X[batch_indices]\n",
        "            y_batch = y[batch_indices]\n",
        "\n",
        "            grad = gradient(X_batch, y_batch, weights)\n",
        "            weights -= learning_rate * grad / batch_size\n",
        "\n",
        "    return weights\n",
        "\n",
        "learning_rate = 0.001\n",
        "num_epochs = 1000\n",
        "\n",
        "# Use batch gradient descent\n",
        "weights = batch_gradient_descent(X_train, y_train, learning_rate, num_epochs, batch_size=32)\n",
        "print(\"Weights:\", weights)\n",
        "\n",
        "y_pred = predict(X_train, weights)\n",
        "y_pred = np.round(y_pred).astype(int)\n",
        "\n",
        "accuracy = calculate_metrics(y_train,y_pred)\n",
        "print(\"\\nThe Accuracy Score of Training Data: \", accuracy)\n",
        "\n",
        "y_pred = predict(X_test, weights)\n",
        "y_pred = np.round(y_pred).astype(int)\n",
        "\n",
        "accuracy = calculate_metrics(y_test,y_pred)\n",
        "print(\"\\nThe Accuracy Score of Test Data: \", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **_Insights drawn (plots, markdown explanations)_**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g7NxS5sSCRv"
      },
      "source": [
        "\n",
        "We will analyze the Insights between SGD and BGD by Looking into the Implementation, Performance and Accuracy.\n",
        "\n",
        "### **Training Algorithm**\n",
        "\n",
        "SDG updates the model weights after processing each individual training example, while BGD updates the weights after processing the entire training dataset.\n",
        "\n",
        "SDG shuffles the training data at the epoch, while in BGD processes all the samples together in each update. Learning in SDG occurs incrementally for each sample, which leads to faster convergence, while the BGD is based on the average gradient of the entire set.\n",
        "\n",
        "### **Coefficients Analysis**\n",
        "\n",
        "#### **SGD**\n",
        "\n",
        "**weights (w)** : [0.4176826, 0.10056565, 0.14534705, 0.0034014, 0.00556761, -0.00441604, 0.07195514, 0.03756527, 0.00074879]\n",
        "\n",
        "#### **BGD**\n",
        "\n",
        "**weights (w)** : [0.3968066, 0.0790767, 0.14720821, 0.00376867, 0.0081181, -0.01657227, 0.06447822, 0.053887,  0.01142693]\n",
        "\n",
        "As the optimization is different, the weights are also different. BGD produces slightly smaller weights compared to SGD. BGD has more stable updates because it considers entire batch in each iteration, while SGD produces noisy updates due to consideration of single random sample at a time.\n",
        "\n",
        "### **Accuracy**\n",
        "\n",
        "#### **SGD**\n",
        "\n",
        "**Training Data Accuracy: ~62%**\n",
        "\n",
        "**Test Data Accuracy: ~58%**\n",
        "\n",
        "#### **BGD**\n",
        "\n",
        "**Training Data Accuracy: ~55%**\n",
        "\n",
        "**Test Data Accuracy: ~61%**\n",
        "\n",
        "SGD performs reasonably well in terms of the training data accuracy, however a noticeable drop is visible for test data accuracy. This suggests that the model might be slightly overfitting the training data. This could be due to the fact that the model is not able to generalize well to the unseen data. BGD on the other hand has the opposite behavior. It achieves a bit lower accuracy for training data, but performs better for test data.\n",
        "\n",
        "BGD provides more stability as it has more stable weight updates. SGD converges faster in terms of iterations, but its path can be noisy and unstable due to the random sampling.\n",
        "\n",
        "### **Comparison and Insights**\n",
        "\n",
        "1.  Surprisingly, BGD performs better for test/unseen data, while SGD performs better for training data.\n",
        "\n",
        "2.  BGD produces smaller weights and has stable weight updates. SGD converges faster but produces noisy updates.\n",
        "\n",
        "3.  The choice between SGD and BGD depends on factors like dataset, model complexity, computational resources and required speed. But generally SGD is preferred over BGD due its speed. Experimenting with the hyperparameters like `learning_rate`, `batch_size` and `num_epochs` can help in finding optimal model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "X = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)\n",
        "\n",
        "# Define BGD and SGD functions\n",
        "def batch_gradient_descent(X, y, learning_rate=0.01, n_iterations=1000):\n",
        "    m = len(X)\n",
        "    theta = np.random.randn(2, 1)\n",
        "    loss_history = []\n",
        "\n",
        "    for iteration in range(n_iterations):\n",
        "        gradients = 2/m * X.T.dot(X.dot(theta) - y)\n",
        "        theta -= learning_rate * gradients\n",
        "        loss = np.mean((X.dot(theta) - y) ** 2)\n",
        "        loss_history.append(loss)\n",
        "\n",
        "    return theta, loss_history\n",
        "\n",
        "def stochastic_gradient_descent(X, y, learning_rate=0.01, n_iterations=1000):\n",
        "    m = len(X)\n",
        "    theta = np.random.randn(2, 1)\n",
        "    loss_history = []\n",
        "\n",
        "    for iteration in range(n_iterations):\n",
        "        random_index = np.random.randint(m)\n",
        "        xi = X[random_index:random_index+1]\n",
        "        yi = y[random_index:random_index+1]\n",
        "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
        "        theta -= learning_rate * gradients\n",
        "        loss = np.mean((X.dot(theta) - y) ** 2)\n",
        "        loss_history.append(loss)\n",
        "\n",
        "    return theta, loss_history\n",
        "\n",
        "# Add a bias term to X\n",
        "X_b = np.c_[np.ones((100, 1)), X]\n",
        "\n",
        "# Perform BGD and record loss history\n",
        "bgd_theta, bgd_losses = batch_gradient_descent(X_b, y)\n",
        "\n",
        "# Perform SGD and record loss history\n",
        "sgd_theta, sgd_losses = stochastic_gradient_descent(X_b, y)\n",
        "\n",
        "# Plot the loss curve over iterations for BGD and SGD\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(len(bgd_losses)), bgd_losses, label='Batch Gradient Descent', color='b')\n",
        "plt.plot(range(len(sgd_losses)), sgd_losses, label='Stochastic Gradient Descent', color='r')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Curve over Iterations')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- Explain your model Implementation using mathematical formulas and algorithms -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2p-mPVFCSe9V"
      },
      "source": [
        "# **_4. Comparison of Lasso and Ridge Regression using Polynomial Regression_**\n",
        "\n",
        "## **_Lasso Regression_**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTjoKWdNSpcE"
      },
      "outputs": [],
      "source": [
        "class LassoRegression:\n",
        "\n",
        "\n",
        "    # defining the constructor with learning rate and no of iterations (Hyperparameters)\n",
        "    def __init__(self, learning_rate, no_of_iterations, alpha):\n",
        "        \"\"\"\n",
        "\t\tThe __init__ function is called when the class is instantiated.\n",
        "\t\tSets up the initial values of all attributes, and it can also do any other setup that might be necessary for \n",
        "\t\tyour object to function properly.\n",
        "\t\t\n",
        "\t\t:param self: Represent the instance of the class\n",
        "\t\t:param learning_rate: Control how much the weights are adjusted each time\n",
        "\t\t:param no_of_iterations: Set the number of iterations for which we want to run the gradient descent algorithm\n",
        "        :param alpha: Regularization parameter\n",
        "\t\t:return: Nothing\n",
        "\t\t\"\"\"\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "        self.no_of_iterations = no_of_iterations\n",
        "        self.alpha = alpha\n",
        "\n",
        "\n",
        "    # model function to train the model with dataset\n",
        "    def model(self, X, Y):\n",
        "        \"\"\"\n",
        "\t\tThe model function is used to train the model.\n",
        "\t\tIt takes in two parameters: X and Y, which are numpy arrays/matrices of shape (m,n) and (m,1) respectively.\n",
        "\t\tThe function updates the weights w and bias b using gradient descent algorithm.\n",
        "\n",
        "\t\t:param self: Represent the instance of the class\n",
        "\t\t:param X: Store the training data\n",
        "\t\t:param Y: Calculate the error and the weights\n",
        "\t\t:return: Nothing\n",
        "\t\t\"\"\"\n",
        "\n",
        "        # Initialize the weights and bias\n",
        "        self.m, self.n = X.shape\n",
        "        self.w = np.zeros(self.n)\n",
        "        self.b = 0\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "\n",
        "        # implementing gradient descent for optimization\n",
        "        for i in range(self.no_of_iterations):\n",
        "            self.update_weights_and_bias()\n",
        "\n",
        "\n",
        "    # function for updating the weights and bias using gradient descent\n",
        "    def update_weights_and_bias(self):\n",
        "        \"\"\"\n",
        "\t\tThe update_weights_and_bias function updates the weights and bias using the gradient descent formula.\n",
        "\t\tThe function takes in no arguments, but uses self.w, self.b, self.alpha, self.X and self.Y to update \n",
        "\t\tthe weights and bias.\n",
        "\n",
        "\t\t:param self: Represent the instance of the class\n",
        "\t\t:return: The updated weights and bias\n",
        "\t\t\"\"\"\n",
        "        \n",
        "        Y_hat = 1 / (1 + np.exp(-(self.X.dot(self.w) + self.b)))\n",
        "        dw = (1/self.m) * np.dot(self.X.T, (Y_hat - self.Y))\n",
        "        db = (1/self.m) * np.sum(Y_hat - self.Y)\n",
        "\n",
        "        # Update the weights and bias using Lasso regularization\n",
        "        self.w = self.w - self.learning_rate * (dw + self.alpha * np.sign(self.w))\n",
        "        self.b = self.b - self.learning_rate * db\n",
        "\n",
        "\n",
        "    # predict function to predict the output using the trained model\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        The predict function takes in a matrix of data points and returns the predicted class for each point.\n",
        "        The predict function calculates the probability that each data point belongs to the positive class, which is \n",
        "        then used to determine if the label should be 1 or 0. If Y_hat > 0.5, then it is classified as 1; \n",
        "        otherwise it is classified as 0.\n",
        "        \n",
        "        :param self: Represent the instance of the class\n",
        "        :param X: Pass the input data to the model\n",
        "        :return: The predicted value of the input x\n",
        "        \"\"\"\n",
        "        \n",
        "        Y_pred = 1 / (1 + np.exp(-(X.dot(self.w) + self.b)))\n",
        "        Y_pred = np.where(Y_pred > 0.5, 1, 0)\n",
        "        return Y_pred\n",
        "\n",
        "\n",
        "    # get_coefficients function to get the coefficients of the trained model\n",
        "    def get_coefficients(self):\n",
        "        \"\"\"\n",
        "        The get_coefficients function returns the weights and bias of the perceptron.\n",
        "        \n",
        "        :param self: Represent the instance of the class\n",
        "        :return: The weights and bias of the perceptron\n",
        "        \"\"\"\n",
        "        return self.w, self.b\n",
        "\n",
        "\n",
        "# Training the model\n",
        "classifierLR = LassoRegression(learning_rate=0.01, no_of_iterations=1000, alpha=0.01)\n",
        "classifierLR.model(x_tr, y_tr)\n",
        "\n",
        "# Get and print the Lasso coefficients\n",
        "lasso_coefficients = classifierLR.get_coefficients()\n",
        "print(\"Lasso Coefficients:\")\n",
        "print(\"Coefficients (w):\", lasso_coefficients[0])\n",
        "print(\"Bias (b):\", lasso_coefficients[1])\n",
        "\n",
        "x_train_predict = classifierLR.predict(x_tr)\n",
        "train_data_accuracy_lasso = accuracy_score(x_train_predict, y_tr)\n",
        "\n",
        "print(\"\\nThe Accuracy Score of Training Data: \", train_data_accuracy_lasso)\n",
        "\n",
        "# # Calculate Mean Squared Error (MSE) manually\n",
        "# mse = np.mean((y_te - x_train_predict) ** 2)\n",
        "\n",
        "# # Calculate R-squared (R2) manually\n",
        "# y_mean = np.mean(y_te)\n",
        "# ss_total = np.sum((y_te - y_mean) ** 2)\n",
        "# ss_residual = np.sum((y_te - x_train_predict) ** 2)\n",
        "# r2 = 1 - (ss_residual / ss_total)\n",
        "\n",
        "# print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "# print(f\"R-squared (R2): {r2}\")\n",
        "\n",
        "# Model Evaluation for Test Data\n",
        "x_test_predict = classifierLR.predict(x_te)\n",
        "test_data_accuracy_lasso = accuracy_score(x_test_predict, y_te)\n",
        "\n",
        "print(\"\\nThe Accuracy Score of Test Data: \", test_data_accuracy_lasso)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr-p_RtzSqUF"
      },
      "source": [
        "## **_Ridge Regression_**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWqMiCzVSuAD"
      },
      "outputs": [],
      "source": [
        "class RidgeRegression:\n",
        "\n",
        "\n",
        "    # defining the constructor with learning rate and no of iterations (Hyperparameters)\n",
        "    def __init__(self, learning_rate, no_of_iterations, alpha):\n",
        "        \"\"\"\n",
        "\t\tThe __init__ function is called when the class is instantiated.\n",
        "\t\tSets up the initial values of all attributes, and it can also do any other setup that might be necessary for \n",
        "\t\tyour object to function properly.\n",
        "\t\t\n",
        "\t\t:param self: Represent the instance of the class\n",
        "\t\t:param learning_rate: Control how much the weights are adjusted each time\n",
        "\t\t:param no_of_iterations: Set the number of iterations for which we want to run the gradient descent algorithm\n",
        "        :param alpha: Regularization parameter\n",
        "\t\t:return: Nothing\n",
        "\t\t\"\"\"\n",
        "        \n",
        "        self.learning_rate = learning_rate\n",
        "        self.no_of_iterations = no_of_iterations\n",
        "        self.alpha = alpha\n",
        "\n",
        "\n",
        "    # model function to train the model with dataset\n",
        "    def model(self, X, Y):\n",
        "        \"\"\"\n",
        "\t\tThe model function is used to train the model.\n",
        "\t\tIt takes in two parameters: X and Y, which are numpy arrays/matrices of shape (m,n) and (m,1) respectively.\n",
        "\t\tThe function updates the weights w and bias b using gradient descent algorithm.\n",
        "\n",
        "\t\t:param self: Represent the instance of the class\n",
        "\t\t:param X: Store the training data\n",
        "\t\t:param Y: Calculate the error and the weights\n",
        "\t\t:return: Nothing\n",
        "\t\t\"\"\"\n",
        "\n",
        "        self.m, self.n = X.shape\n",
        "        self.w = np.zeros(self.n)\n",
        "        self.b = 0\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "\n",
        "        for i in range(self.no_of_iterations):\n",
        "            self.update_weights_and_bias()\n",
        "\n",
        "\n",
        "    # function for updating the weights and bias using gradient descent\n",
        "    def update_weights_and_bias(self):\n",
        "        \"\"\"\n",
        "\t\tThe update_weights_and_bias function updates the weights and bias using the gradient descent formula.\n",
        "\t\tThe function takes in no arguments, but uses self.w, self.b, self.alpha, self.X and self.Y to update \n",
        "\t\tthe weights and bias.\n",
        "\n",
        "\t\t:param self: Represent the instance of the class\n",
        "\t\t:return: The updated weights and bias\n",
        "\t\t\"\"\"\n",
        "\n",
        "        Y_hat = 1 / (1 + np.exp(-(self.X.dot(self.w) + self.b)))\n",
        "        dw = (1/self.m) * np.dot(self.X.T, (Y_hat - self.Y))\n",
        "        db = (1/self.m) * np.sum(Y_hat - self.Y)\n",
        "\n",
        "        # Update the weights and bias using Ridge regularization\n",
        "        self.w = self.w - self.learning_rate * (dw + 2 * self.alpha * self.w)\n",
        "        self.b = self.b - self.learning_rate * db\n",
        "\n",
        "    # predict function to predict the output using the trained model\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        The predict function takes in a matrix of data points and returns the predicted class for each point.\n",
        "        The predict function calculates the probability that each data point belongs to the positive class, which is \n",
        "        then used to determine if the label should be 1 or 0. If Y_hat > 0.5, then it is classified as 1; \n",
        "        otherwise it is classified as 0.\n",
        "        \n",
        "        :param self: Represent the instance of the class\n",
        "        :param X: Pass the input data to the model\n",
        "        :return: The predicted value of the input x\n",
        "        \"\"\"\n",
        "        \n",
        "        Y_pred = 1 / (1 + np.exp(-(X.dot(self.w) + self.b)))\n",
        "        Y_pred = np.where(Y_pred > 0.5, 1, 0)\n",
        "        return Y_pred\n",
        "\n",
        "\n",
        "    # get_coefficients function to get the coefficients of the trained model\n",
        "    def get_coefficients(self):\n",
        "        \"\"\"\n",
        "        The get_coefficients function returns the weights and bias of the perceptron.\n",
        "        \n",
        "        :param self: Represent the instance of the class\n",
        "        :return: The weights and bias of the perceptron\n",
        "        \"\"\"\n",
        "        return self.w, self.b\n",
        "\n",
        "\n",
        "# Training the model\n",
        "classifierRR = RidgeRegression(learning_rate=0.01, no_of_iterations=1000, alpha=0.01)\n",
        "classifierRR.model(x_tr, y_tr)\n",
        "\n",
        "# Get and print the Ridge coefficients\n",
        "ridge_coefficients = classifierRR.get_coefficients()\n",
        "print(\"Ridge Coefficients:\")\n",
        "print(\"Coefficients (w):\", ridge_coefficients[0])\n",
        "print(\"Bias (b):\", ridge_coefficients[1])\n",
        "\n",
        "x_train_predict = classifierRR.predict(x_tr)\n",
        "train_data_accuracy_ridge = accuracy_score(x_train_predict, y_tr)\n",
        "\n",
        "print(\"\\nThe Accuracy Score of Training Data: \", train_data_accuracy_ridge)\n",
        "\n",
        "# # Calculate Mean Squared Error (MSE) manually\n",
        "# mse = np.mean((y_te - x_train_predict) ** 2)\n",
        "\n",
        "# # Calculate R-squared (R2) manually\n",
        "# y_mean = np.mean(y_te)\n",
        "# ss_total = np.sum((y_te - y_mean) ** 2)\n",
        "# ss_residual = np.sum((y_te - x_train_predict) ** 2)\n",
        "# r2 = 1 - (ss_residual / ss_total)\n",
        "\n",
        "# print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "# print(f\"R-squared (R2): {r2}\")\n",
        "\n",
        "# Model Evaluation for Test Data\n",
        "x_test_predict = classifierRR.predict(x_te)\n",
        "test_data_accuracy_ridge = accuracy_score(x_test_predict, y_te)\n",
        "\n",
        "print(\"\\nThe Accuracy Score of Test Data: \", test_data_accuracy_ridge)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlUMAWsiSug9"
      },
      "source": [
        "## **_Insights drawn (plots, markdown explanations)_**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "We will analyze the Insights between Lasso Regression and Ridge Regression by Looking into the Implementation, Performance and Accuracy.\n",
        "\n",
        "### **Coefficients Analysis**\n",
        "\n",
        "#### **_Lasso Regression_**\n",
        "\n",
        "**Coefficients (w)**: [ 4.02520261e-01, 4.87138065e-01, 4.91059642e-05, 8.45477045e-05, -1.09326859e-04, 2.61785942e-01, 2.55982811e-01, 2.68001219e-02]\n",
        "\n",
        "**Bias (b)**: -0.3044113973314351\n",
        "\n",
        "#### **_Ridge Regression_**\n",
        "\n",
        "**Coefficients (w)**: [ 0.4109134, 0.48237903, 0.00631716, -0.0038123, 0.0035613, 0.27685469, 0.2791599, 0.05583049]\n",
        "\n",
        "**Bias (b)**: -0.3066060451645528\n",
        "\n",
        "\n",
        "Both Lasso and Ridge Regression introduce regularization terms to control the magnitude of Coefficients. Lasso uses L1 regularization which tends to sparse coefficient vectors, while Ridge uses L2 regularization which tends to produce smaller but non-zero coefficients.\n",
        "\n",
        "### **Model Performance Analysis**\n",
        "\n",
        "#### **_Lasso Regression_**\n",
        "\n",
        "**Training Data Accuracy: ~70.5%**\n",
        "\n",
        "**Test Data Accuracy: ~70%**\n",
        "\n",
        "#### **_Ridge Regression_**\n",
        "\n",
        "**Training Data Accuracy: ~69.5%**\n",
        "\n",
        "**Test Data Accuracy: ~71%**\n",
        "\n",
        "### **Comparison and Insights**\n",
        "\n",
        "1.  Both models performed similarly on the test data, with test accuracies of around 70%. Ridge Regression performs slightly better compared to Lasso Regression.\n",
        "\n",
        "2.  Lasso Regression tend to produce a more sparse coefficient vector (some very near to zero), indicating feature selection. Ridge produces smaller but non-zero coefficients. Both have nearly same Bias.\n",
        "\n",
        "3.  The choice between Lasso and Ridge Regression depends on the specific requirements of the problem. If the solution requires to retain all the features with smaller coefficients then Ridge is recommended. If the solution requires to perform feature selection then Lasso is recommended.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "X = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)\n",
        "\n",
        "# Define Lasso and Ridge functions\n",
        "def lasso_regression(X, y, alpha, learning_rate=0.01, n_iterations=1000):\n",
        "    m = len(X)\n",
        "    theta = np.random.randn(2, 1)\n",
        "    loss_history = []\n",
        "\n",
        "    for iteration in range(n_iterations):\n",
        "        gradients = 2/m * X.T.dot(X.dot(theta) - y) + alpha * np.sign(theta)\n",
        "        theta -= learning_rate * gradients\n",
        "        loss = np.mean((X.dot(theta) - y) ** 2)\n",
        "        loss_history.append(loss)\n",
        "\n",
        "    return theta, loss_history\n",
        "\n",
        "def ridge_regression(X, y, alpha, learning_rate=0.01, n_iterations=1000):\n",
        "    m = len(X)\n",
        "    theta = np.random.randn(2, 1)\n",
        "    loss_history = []\n",
        "\n",
        "    for iteration in range(n_iterations):\n",
        "        gradients = 2/m * X.T.dot(X.dot(theta) - y) + 2 * alpha * theta\n",
        "        theta -= learning_rate * gradients\n",
        "        loss = np.mean((X.dot(theta) - y) ** 2)\n",
        "        loss_history.append(loss)\n",
        "\n",
        "    return theta, loss_history\n",
        "\n",
        "# Add a bias term to X\n",
        "X_b = np.c_[np.ones((100, 1)), X]\n",
        "\n",
        "# Perform Lasso regression and record loss history\n",
        "lasso_alpha = 0.1\n",
        "lasso_theta, lasso_losses = lasso_regression(X_b, y, alpha=lasso_alpha)\n",
        "\n",
        "# Perform Ridge regression and record loss history\n",
        "ridge_alpha = 0.1\n",
        "ridge_theta, ridge_losses = ridge_regression(X_b, y, alpha=ridge_alpha)\n",
        "\n",
        "# Plot the loss curve over iterations for Lasso and Ridge\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(len(lasso_losses)), lasso_losses, label=f'Lasso (Alpha={lasso_alpha})', color='b')\n",
        "plt.plot(range(len(ridge_losses)), ridge_losses, label=f'Ridge (Alpha={ridge_alpha})', color='r')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Curve over Iterations for Lasso and Ridge')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Plot Lasso Coefficients\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(np.arange(len(lasso_coefficients[0])), lasso_coefficients[0])\n",
        "plt.xlabel(\"Coefficient Index\")\n",
        "plt.ylabel(\"Coefficient Value\")\n",
        "plt.title(\"Lasso Coefficients\")\n",
        "\n",
        "# Plot Ridge Coefficients\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(np.arange(len(ridge_coefficients[0])), ridge_coefficients[0])\n",
        "plt.xlabel(\"Coefficient Index\")\n",
        "plt.ylabel(\"Coefficient Value\")\n",
        "plt.title(\"Ridge Coefficients\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot Accuracy Scores\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar([\"Lasso\", \"Ridge\"], [train_data_accuracy_lasso, train_data_accuracy_ridge])\n",
        "plt.ylabel(\"Accuracy Score\")\n",
        "plt.title(\"Accuracy Score for Training Data (Lasso vs. Ridge)\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzSfpXtdS_q0"
      },
      "source": [
        "# **_5. Comparison of Logistic Regression and Least Squares Classification_**\n",
        "\n",
        "## **_Logistic Regression_**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLCup6QlTG0G"
      },
      "outputs": [],
      "source": [
        "class Logistic_Regression():\n",
        "\t\n",
        "\n",
        "\t# defining the constructor with learning rate and no of iterations (Hyperparameters)\n",
        "\tdef __init__(self, learning_rate, no_of_iterations):\n",
        "\t\t\"\"\"\n",
        "\t\tThe __init__ function is called when the class is instantiated.\n",
        "\t\tSets up the initial values of all attributes, and it can also do any other setup that might be necessary for \n",
        "\t\tyour object to function properly.\n",
        "\t\t\n",
        "\t\t:param self: Represent the instance of the class\n",
        "\t\t:param learning_rate: Control how much the weights are adjusted each time\n",
        "\t\t:param no_of_iterations: Set the number of iterations for which we want to run the gradient descent algorithm\n",
        "\t\t:return: Nothing\n",
        "\t\t\"\"\"\n",
        "        \n",
        "\t\tself.learning_rate = learning_rate\n",
        "\t\tself.no_of_iterations = no_of_iterations\n",
        "\n",
        "\n",
        "\n",
        "    # model function to train the model with dataset\n",
        "\tdef model(self, X, Y):\n",
        "\t\t\"\"\"\n",
        "\t\tThe model function is used to train the model.\n",
        "\t\tIt takes in two parameters: X and Y, which are numpy arrays/matrices of shape (m,n) and (m,1) respectively.\n",
        "\t\tThe function updates the weights w and bias b using gradient descent algorithm.\n",
        "\n",
        "\t\t:param self: Represent the instance of the class\n",
        "\t\t:param X: Store the training data\n",
        "\t\t:param Y: Calculate the error and the weights\n",
        "\t\t:return: Nothing\n",
        "\t\t\"\"\"\n",
        "\t\t\n",
        "\t\t# number of data points(rows) = m and no of features(columns) = n\n",
        "\t\tself.m, self.n = X.shape\n",
        "\n",
        "\t\t# initializing the weights and bias to zero\n",
        "\t\tself.w = np.zeros(self.n)\n",
        "\t\tself.b = 0\n",
        "\t\tself.X = X\n",
        "\t\tself.Y = Y\n",
        "\n",
        "\t\t# implementing gradient descent for optimization\n",
        "\t\tfor i in range(self.no_of_iterations):\n",
        "\t\t\tself.update_weights_and_bias()\n",
        "\n",
        "\n",
        "\t# function for updating the weights and bias using gradient descent\n",
        "\tdef update_weights_and_bias(self):\n",
        "\t\t\"\"\"\n",
        "\t\tThe update_weights_and_bias function updates the weights and bias using the gradient descent formula.\n",
        "\t\tThe function takes in no arguments, but uses self.w, self.b, self.X and self.Y to update \n",
        "\t\tthe weights and bias.\n",
        "\n",
        "\t\t:param self: Represent the instance of the class\n",
        "\t\t:return: The updated weights and bias\n",
        "\t\t\"\"\"\n",
        "\t\t\n",
        "\t\t# weights are updated using the formula w := w - learning_rate * dw\n",
        "\t\t# bias is updated using the formula b := b - learning_rate * db\n",
        "\n",
        "        # Y_hat formula (sigmoid function) = w.X + b\n",
        "\t\tY_hat = 1 / (1 + np.exp(-(self.X.dot(self.w) + self.b)))\n",
        "        \n",
        "        # derivatives\n",
        "\t\tdw = (1/self.m)*np.dot(self.X.T, (Y_hat - self.Y))\n",
        "\t\tdb = (1/self.m)*np.sum(Y_hat - self.Y)\n",
        "\n",
        "\t\t# updating the weights and bias using the gradient descent formula\n",
        "\t\tself.w = self.w - self.learning_rate * dw\n",
        "\t\tself.b = self.b - self.learning_rate * db\n",
        "\n",
        "\n",
        "\n",
        "\t# predict function to predict the output using Sigmoid Equation and Decision Boundary\n",
        "\tdef predict(self, X):\n",
        "\t\t\"\"\"\n",
        "\t\tThe predict function takes in a matrix of features and returns the predicted labels for each row.\n",
        "\t\tThe predict function uses the sigmoid function to calculate Y_hat, which is then used to determine if \n",
        "\t\tthe label should be 1 or 0. If Y_hat > 0.5, then it is classified as 1; otherwise it is classified as 0.\n",
        "\n",
        "\t\t:param self: Represent the instance of the class\n",
        "\t\t:param X: Pass the input data to the model\n",
        "\t\t:return: The predicted values of y (vector) for the given x\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\t# predicting the output by checking Y_hat > 0.5 for 1 and Y_hat <= 0.5 for 0\n",
        "\t\tY_pred = 1 / (1 + np.exp(-(X.dot(self.w) + self.b)))\n",
        "\t\tY_pred = np.where(Y_pred > 0.5, 1, 0)\n",
        "\t\treturn Y_pred\n",
        "\n",
        "# Training the model\n",
        "classifierLR = Logistic_Regression(learning_rate=0.01, no_of_iterations=1000)\n",
        "classifierLR.model(x_tr, y_tr)\n",
        "\n",
        "# Model Evaluation\n",
        "\n",
        "# Model Evaluation for Training Data\n",
        "x_train_predict = classifierLR.predict(x_tr)\n",
        "train_data_accuracy = accuracy_score(x_train_predict, y_tr)\n",
        "\n",
        "print(\"\\nThe Accuracy Score of Training Data: \", train_data_accuracy)\n",
        "\n",
        "# Model Evaluation for Test Data\n",
        "x_test_predict = classifierLR.predict(x_te)\n",
        "test_data_accuracy = accuracy_score(x_test_predict, y_te)\n",
        "\n",
        "print(\"\\nThe Accuracy Score of Test Data: \", test_data_accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdX2kQ-nTHXc"
      },
      "source": [
        "## **_Least Squares Classification_**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7qJtwGnTMdb"
      },
      "outputs": [],
      "source": [
        "class LMSClassifier:\n",
        "\n",
        "\n",
        "    # defining the constructor with learning rate and no of iterations (Hyperparameters)\n",
        "    def __init__(self, learning_rate, no_of_iterations):\n",
        "        \"\"\"\n",
        "\t\tThe __init__ function is called when the class is instantiated.\n",
        "\t\tSets up the initial values of all attributes, and it can also do any other setup that might be necessary for \n",
        "\t\tyour object to function properly.\n",
        "\t\t\n",
        "\t\t:param self: Represent the instance of the class\n",
        "\t\t:param learning_rate: Control how much the weights are adjusted each time\n",
        "\t\t:param no_of_iterations: Set the number of iterations for which we want to run the gradient descent algorithm\n",
        "\t\t:return: Nothing\n",
        "\t\t\"\"\"\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "        self.no_of_iterations = no_of_iterations\n",
        "\n",
        "\n",
        "    # model function to train the model with dataset\n",
        "    def model(self, X, Y):\n",
        "        \"\"\"\n",
        "\t\tThe model function is used to train the model.\n",
        "\t\tIt takes in two parameters: X and Y, which are numpy arrays/matrices of shape (m,n) and (m,1) respectively.\n",
        "\t\tThe function updates the weights w and bias b using gradient descent algorithm.\n",
        "\n",
        "\t\t:param self: Represent the instance of the class\n",
        "\t\t:param X: Store the training data\n",
        "\t\t:param Y: Calculate the error and the weights\n",
        "\t\t:return: Nothing\n",
        "\t\t\"\"\"\n",
        "        \n",
        "        no_samples, no_features = X.shape\n",
        "        X = X.to_numpy()\n",
        "        Y = Y.to_numpy()\n",
        "        \n",
        "        # Initializing array of ones\n",
        "        X1 = np.ones(no_samples)\n",
        "        X1 = X1.reshape(1,no_samples)\n",
        "        \n",
        "        # Transform and concatenate columns of the input array X into the new array X1\n",
        "        for j in range(no_features):\n",
        "            x_i = X[:, j].reshape(-1, 1)\n",
        "            x_i = x_i.T\n",
        "            X1 = np.concatenate((X1, x_i), axis=0)\n",
        "        \n",
        "        Y = Y.reshape(-1, 1)\n",
        "        X1 = X1.T\n",
        "        \n",
        "        # Compute the coefficients (beta) for linear regression using the pseudo-inverse method\n",
        "        self.beta = np.linalg.pinv(X1.T @ X1) @ (X1.T @ Y)\n",
        "\n",
        "\n",
        "    # predict function to predict the output using the coefficients (beta)\n",
        "    def predict(self, X_test):\n",
        "        \"\"\"\n",
        "        The predict function takes in a matrix of test data and returns the predicted values for each sample.\n",
        "        The function first adds a column of ones to the test data, then multiplies it by beta to get Y_hat.\n",
        "        It then reshapes Y_hat into an array with one row and no_samples columns, which is used to calculate mean. \n",
        "        Then it loops through all elements in Y_hat and sets them equal to 0 if they are less than mean or 1 otherwise.\n",
        "        \n",
        "        :param self: Represent the instance of the class\n",
        "        :param X_test: Pass the test data to the predict function\n",
        "        :return: A vector of predictions Y_pred given X\n",
        "        \"\"\"\n",
        "        \n",
        "        no_samples, no_features = X_test.shape\n",
        "        X = np.concatenate((np.ones((no_samples, 1)), X_test), axis=1)\n",
        "        Y_hat = X @ self.beta\n",
        "        \n",
        "        # Calculate mean of Y_hat\n",
        "        Y_hat = Y_hat.reshape(no_samples, )\n",
        "        mean = np.mean(Y_hat)\n",
        "        Y_hat = Y_hat.reshape(1, no_samples)\n",
        "        \n",
        "\t\t# predicting the output by checking Y_hat > mean for 1 and Y_hat <= mean for 0\n",
        "        for j in range(no_samples):\n",
        "            if Y_hat[0, j] > mean:\n",
        "                Y_hat[0, j] = 1\n",
        "            else:\n",
        "                Y_hat[0, j] = 0\n",
        "        Y_pred = Y_hat.reshape(no_samples, )\n",
        "        return Y_pred\n",
        "\n",
        "# Training the model\n",
        "classifierLMS = LMSClassifier(learning_rate=0.01, no_of_iterations=1000)\n",
        "classifierLMS.model(x_tr, y_tr)\n",
        "classifierLMS.beta\n",
        "\n",
        "# Model Evaluation for Training Data\n",
        "x_train_predict = classifierLMS.predict(x_tr)\n",
        "train_data_accuracy = accuracy_score(x_train_predict, y_tr)\n",
        "\n",
        "print(\"\\nThe Accuracy Score of Training Data: \", train_data_accuracy)\n",
        "\n",
        "# Model Evaluation for Test Data\n",
        "x_test_predict = classifierLMS.predict(x_te)\n",
        "test_data_accuracy = accuracy_score(x_test_predict, y_te)\n",
        "\n",
        "print(\"\\nThe Accuracy Score of Training Data: \", test_data_accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSoa7KO1TM6-"
      },
      "source": [
        "## **_Insights drawn (plots, markdown explanations)_**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- Explain your model Implementation using mathematical formulas and algorithms -->\n",
        "\n",
        "We will analyze the Insights between Logistic Regression and Least Mean Square Classification by Looking into the Implementation, Performance and Plot the Decision Boundary for the Model\n",
        "\n",
        "### **Logistic Regression**\n",
        "\n",
        "#### **Implementation**\n",
        "\n",
        "Logistic Regression is implemented as a class with methods for training and prediction. The key components includes:\n",
        "\n",
        "1.  Constructor (`__init__`) to set the learning rate and no of iterations.\n",
        "2.  `model` method to train the model using gradient descent.\n",
        "3.  `update_weights_and_bias` method for updating the weights and bias.\n",
        "4.  `predict` method to make predictions.\n",
        "\n",
        "#### **Model Training**\n",
        "\n",
        "Logistic Regression is trained using the sigmoid function to calculate the predicted values. It uses gradient descent to update weights and bias iteratively.\n",
        "\n",
        "#### **Model Evaluation**\n",
        "\n",
        "After Training, the model is evaluated using the accuracy score to measure the performance of the model. it is evaluated on both training and test data.\n",
        "\n",
        "**Training Data Accuracy: ~70%**\n",
        "\n",
        "**Test Data Accuracy: ~74%**\n",
        "\n",
        "\n",
        "### **Least Mean Square Classification**\n",
        "\n",
        "#### **Implementation**\n",
        "\n",
        "Least Mean Square Classification is implemented as a class with methods for training and prediction. The key components include:\n",
        "\n",
        "1.  Constructor (`__init__`) to set the learning rate and no of iterations.\n",
        "2.  `mode1` method to train the model using the pseudo-inverse method\n",
        "3.  `predict` method for making predictions.\n",
        "\n",
        "#### **Model Training**\n",
        "\n",
        "LMS Classification computes the coefficients (beta) for linear regression using the pseudo-inverse method.\n",
        "\n",
        "#### **Evaluation**\n",
        "\n",
        "After training, the model is evaluated on both the training and test data using accuracy scores.\n",
        "\n",
        "**Training Data Accuracy: ~66.5%**\n",
        "\n",
        "**Test Data Accuracy: ~67%**\n",
        "\n",
        "### **Comparison and Insights**\n",
        "\n",
        "1.  Logistic Regression generally outperforms LMS Classification in terms of accuracy.\n",
        "\n",
        "2.  Logistic Regression is better suited for binary classification problems, especially when the relationship between features and targets is not linear as it is based on modelling the probability of target variable belonging to certain class. LMS Classification is based on Linear Regression hence does'nt perform well on binary classification problems.\n",
        "\n",
        "3.  The choice of hyperparameters can significantly impact the performance of both methods. by fine-tuning them we can achieve better results. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Graph plotted to visualize the comparison of the models\n",
        "x_te = x_te.to_numpy()\n",
        "y_te = y_te.to_numpy()\n",
        "\n",
        "class_0 = x_te[y_te == 0]\n",
        "class_1 = x_te[y_te == 1]\n",
        "\n",
        "plt.scatter(class_0[:, 0], class_0[:, 1], label=\"Class 0\", marker='o', c='blue')\n",
        "plt.scatter(class_1[:, 0], class_1[:, 1], label=\"Class 1\", marker='o', c='red')\n",
        "\n",
        "plt.title(\"Scatter Plot of Test Data\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81SwQ_y4TaSc"
      },
      "source": [
        "# **_5. References_**\n",
        "\n",
        "1.   SDV: https://docs.sdv.dev/sdv/\n",
        "2.   Preprocessing: https://towardsdatascience.com/data-preprocessing-and-eda-for-data-science-50ba6ea65c0a\n",
        "3.   Preprocessing for Missing Data using Pandas: https://pandas.pydata.org/docs/user_guide/missing_data.html\n",
        "4.   EDA using Seaborn: https://www.analyticsvidhya.com/blog/2021/08/how-to-perform-exploratory-data-analysis-a-guide-for-beginners/\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
