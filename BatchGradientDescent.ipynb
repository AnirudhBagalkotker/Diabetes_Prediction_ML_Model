import numpy as np

def batch_gradient_descent(X, Y, w, b, learning_rate, batch_size):
    m, n = X.shape
    dw = np.zeros(n)
    db = 0

    for i in range(0, m, batch_size):
        X_batch = X[i:i+batch_size]
        Y_batch = Y[i:i+batch_size]

        Y_hat_batch = 1 / (1 + np.exp(-(X_batch.dot(w) + b)))

        dw_batch = (1/batch_size) * X_batch.T.dot(Y_hat_batch - Y_batch)
        db_batch = (1/batch_size) * np.sum(Y_hat_batch - Y_batch)

        dw += dw_batch
        db += db_batch

    w = w - learning_rate * dw
    b = b - learning_rate * db

    return w, b
