{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_Regression():\n",
    "\n",
    "\t\"\"\"\n",
    "\tThe __init__ function is called when the class is instantiated.\n",
    "\tIt sets up the initial values of all attributes, and it can also do any other setup that might be necessary for your object to function properly.\n",
    "\t\n",
    "\t:param self: Represent the instance of the class\n",
    "\t:param learning_rate: Control how much the weights are adjusted each time\n",
    "\t:param no_of_iterations: Set the number of iterations for which we want to run the gradient descent algorithm\n",
    "\t:return: Nothing\n",
    "\t\"\"\"\n",
    "\t# defining the constructor with learning rate and no of iterations (Hyperparameters)\n",
    "\tdef __init__(self, learning_rate, no_of_iterations):\n",
    "        \n",
    "\t\tself.learning_rate = learning_rate\n",
    "\t\tself.no_of_iterations = no_of_iterations\n",
    "\n",
    "\n",
    "\t\"\"\"\n",
    "\tThe fit function is used to train the model.\n",
    "\tIt takes in two parameters: X and Y, which are numpy arrays(matrix) of shape (m,n) and (m,1) respectively.\n",
    "\tThe function updates the weights w and bias b using gradient descent algorithm.\n",
    "\n",
    "\t:param self: Represent the instance of the class\n",
    "\t:param X: Store the training data\n",
    "\t:param Y: Calculate the error and the weights\n",
    "\t:return: Nothing\n",
    "\t\"\"\"\n",
    "    # fit function to train the model with dataset\n",
    "\tdef fit(self, X, Y):\n",
    "\t\n",
    "\t\t# number of data points(rows) = m and no of features(columns) = n\n",
    "\t\tself.m, self.n = X.shape\n",
    "\n",
    "\t\t# initializing the weights and bias to zero\n",
    "\t\tself.w = np.zeros(self.n)\n",
    "\t\tself.b = 0\n",
    "\t\tself.X = X\n",
    "\t\tself.Y = Y\n",
    "\n",
    "\t\t# implementing gradient descent for optimization\n",
    "\t\tfor i in range(self.no_of_iterations):\n",
    "\t\t\tself.update_weights_and_bias()\n",
    "\n",
    "\n",
    "\t\"\"\"\n",
    "\tThe update_weights_and_bias function updates the weights and bias using the gradient descent formula.\n",
    "\tThe function takes in no arguments, but uses self.w, self.b, self.X and self.Y to update \n",
    "\tthe weights and bias.\n",
    "\n",
    "\t:param self: Represent the instance of the class\n",
    "\t:return: The updated weights and bias\n",
    "\t\"\"\"\n",
    "\t# function for updating the weights and bias using gradient descent\n",
    "\tdef update_weights_and_bias(self):\n",
    "\t\n",
    "\t\t# weights are updated using the formula w := w - learning_rate * dw\n",
    "\t\t# bias is updated using the formula b := b - learning_rate * db\n",
    "\n",
    "        # Y_hat formula (sigmoid function) = w.X + b\n",
    "\t\tY_hat = 1 / (1 + np.exp(-(self.X.dot(self.w) + self.b)))\n",
    "        \n",
    "        # derivatives\n",
    "\t\tdw = (1/self.m)*np.dot(self.X.T, (Y_hat - self.Y))\n",
    "\t\tdb = (1/self.m)*np.sum(Y_hat - self.Y)\n",
    "\n",
    "\t\t# updating the weights and bias using the gradient descent formula\n",
    "\t\tself.w = self.w - self.learning_rate * dw\n",
    "\t\tself.b = self.b - self.learning_rate * db\n",
    "\n",
    "\n",
    "\t\"\"\"\n",
    "\tThe predict function takes in a matrix of features and returns the predicted labels for each row.\n",
    "\tThe predict function uses the sigmoid function to calculate Y_hat, which is then used to determine if \n",
    "\tthe label should be 1 or 0. If Y_hat > 0.5, then it is classified as 1; otherwise it is classified as 0.\n",
    "\n",
    "\t:param self: Represent the instance of the class\n",
    "\t:param X: Pass the input data to the model\n",
    "\t:return: The predicted values of y for the given x\n",
    "\t\"\"\"\n",
    "\t# predict function to predict the output using Sigmoid Equation and Decision Boundary\n",
    "\tdef predict(self, X):\n",
    "\t\n",
    "\t\t# predicting the output by checking Y_hat > 0.5 for 1 and Y_hat <= 0.5 for 0\n",
    "\t\tY_pred = 1 / (1 + np.exp(-(X.dot(self.w) + self.b)))\n",
    "\t\tY_pred = np.where(Y_pred > 0.5, 1, 0)\n",
    "\t\treturn Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "classifier = Logistic_Regression(learning_rate=0.01, no_of_iterations=1000)\n",
    "classifier.fit(x_tr, y_tr)\n",
    "\n",
    "# Model Evaluation\n",
    "\n",
    "# Model Evaluation for Training Data\n",
    "x_train_predict = classifier.predict(x_tr)\n",
    "train_data_accuracy = accuracy_score(x_train_predict, y_tr)\n",
    "\n",
    "# Model Evaluation for Test Data\n",
    "x_test_predict = classifier.predict(x_te)\n",
    "test_data_accuracy = accuracy_score(x_test_predict, y_te)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
